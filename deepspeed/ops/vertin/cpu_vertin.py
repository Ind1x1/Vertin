# Copyright (c) Microsoft Corporation.
# SPDX-License-Identifier: Apache-2.0

# Sonnet

import torch
from cpuinfo import get_cpu_info
from deepspeed.utils import logger
from deepspeed.utils.logging import should_log_le
from deepspeed.ops.op_builder import CPUVertinBuilder

class SonnetVertinCPUAdam(torch.optim.Optimizer):
    optimizer_id = 0

    def __init__(self,
                 model_params,
                 lr=1e-3,
                 bias_correction=True,
                 betas=(0.9, 0.999),
                 eps=1e-8,
                 weight_decay=0,
                 amsgrad=False,
                 adamw_mode=True,
                 fp32_optimizer_states=True):
    
        default_args = dict(lr=lr,
                            betas=betas,
                            eps=eps,
                            weight_decay=weight_decay,
                            bias_correction=bias_correction,
                            amsgrad=amsgrad)
        super(SonnetVertinCPUAdam, self).__init__(model_params, default_args)

        cpu_info = get_cpu_info()
        self.cpu_vendor = cpu_info["vendor_id_raw"].lower() if "vendor_id_raw" in cpu_info else "unknown"
        if "amd" in self.cpu_vendor:
            for group_id, group in enumerate(self.param_groups):
                for param_id, p in enumerate(group['params']):
                    if p.dtype == torch.half:
                        logger.warning("FP16 param for CPUADAM may not work on AMD CPUs")
                        break
                else:
                    continue
                break

        self.opt_id = SonnetVertinCPUAdam.optimizer_id
        SonnetVertinCPUAdam.optimizer_id = SonnetVertinCPUAdam.optimizer_id + 1
        self.adam_w_mode = adamw_mode
        self.fp32_optimizer_states = fp32_optimizer_states
        self.sn_opt_adam = CPUVertinBuilder().load()

        self.sn_opt_adam.create_vertin(self.opt_id, lr, betas[0], betas[1], eps, weight_decay, adamw_mode, 
                                       should_log_le("info"))
    
    def __del__(self):
        self.sn_opt_adam.destroy_vertin(self.opt_id)

    def __setstate__(self, state):
        super(SonnetVertinCPUAdam, self).__setstate__(state)
        for group in self.param_groups:
            group.setdefault('amsgrad', False)

    @torch.no_grad()
    def step(self, closure=None):
        
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        # intended device for step
        device = torch.device('cpu')

        for group_id, group in enumerate(self.param_groups):
            for param_id, p in enumerate(group['params']):

                if p.grad is None:
                    continue

                assert p.device == device, f"CPUAdam param is on {p.device} and must be 'cpu', make " \
                        "sure your enabled 'offload_optimizer': 'cpu' in your ZeRO config."
                
                state = self.state[p]
                # State initialization
                if len(state) ==0:
                    #print(f'group {group_id} param {param_id} = {p.numel()}')
                    state['step'] = 0

                    #use full precision by default unless self.fp32_optimizer_states is off
                    state_dtype = torch.float if self.fp32_optimizer_states else p.dtype

                    # gradient momentums
                    state['exp_avg'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)
                    #memory_format=torch.preserve_format)
                    # gradient variances
                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)
                    #memory_format=torch.preserve_format)
                
            state['step'] += 1
            beta1, beta2 = group['betas']
            
            self.sn_opt_adam.vertin_update(self.opt_id, state['step'], group['lr'], beta1, beta2, group['eps'],
                                           group['weight_decay'], group['bias_correction'], p.grad.data, state['exp_avg'], state['exp_avg_sq'])

        return loss